{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report for CS598 DL4H in Spring 2023\n",
    "\n",
    "{anevala2, yutaron2}@illinois.edu\n",
    "\n",
    "- Group ID: 64\n",
    "- Paper ID: 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the report and findings\n",
    "\n",
    "A summary of the report and findings (Reproducibility Summary, about 200 words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the MIMIC Ⅲ dataset\n",
    "\n",
    "An overview of the data with any helpful charts and visualizations from the report and ideally directly using the dataset in the notebook (maybe link to the data folder or URL)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three CSV files from the MIMIC Ⅲ dataset.\n",
    "\n",
    "- ADMISSIONS.csv\n",
    "- DIAGNOSES_ICD.csv\n",
    "- ccs_multi_dx_tool_2015.csv\n",
    "\n",
    "We preprocessed these these files to generate \n",
    "- `output.hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- `output2.seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
    "- `output2.types`: contains the map from ICD9 codes to ICD-9 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of patients:', 5447)\n",
      "('number of heart failure patients:', 1280)\n",
      "ratio of heart failure patients:       0.23\n",
      "('number of unique ICD9 codes:', 4512)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./output/output.hfs', 'rb') as f:\n",
    "\thfs = pickle.load(f)\n",
    "with open('./output2/output2.seqs', 'rb') as f:\n",
    "\tseqs = pickle.load(f)\n",
    "with open('./output2/output2.types', 'rb') as f:\n",
    "\ttypes = pickle.load(f)\n",
    "\n",
    "print(\"number of patients:\", len(hfs))\n",
    "print(\"number of heart failure patients:\", sum(hfs))\n",
    "print(\"ratio of heart failure patients: %10.2f\" % (float(sum(hfs)) / len(hfs)))\n",
    "print(\"number of unique ICD9 codes:\", len(types))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we prepared ancestor information from medical ontologies data to produce:\n",
    "\n",
    "- output2.level1.pk\n",
    "- output2.level2.pk\n",
    "- output2.level3.pk\n",
    "- output2.level4.pk\n",
    "- output2.level5.pk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of methodology and experiments\n",
    "\n",
    "An overview of the methodology and experiments run,ideally with executable code examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the GRAM model for heart faiure prediction in `gram_hf.py`. The following code imported it as `gram_hf` and train the model with 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building the model ...  done!!\n",
      "Constructing the optimizer ...  done!!\n",
      "Loading data ...  done!!\n",
      "Optimization start !!\n",
      "Epoch:0, Duration:12.177816, Train_Cost:0.647768, Valid_Cost:0.536243, Test_Cost:0.493310\n",
      "Epoch:1, Duration:12.339224, Train_Cost:0.570731, Valid_Cost:0.518706, Test_Cost:0.486453\n",
      "Epoch:2, Duration:12.307639, Train_Cost:0.534188, Valid_Cost:0.498042, Test_Cost:0.445167\n",
      "Epoch:3, Duration:12.324813, Train_Cost:0.511874, Valid_Cost:0.485034, Test_Cost:0.437686\n",
      "Epoch:4, Duration:16.401006, Train_Cost:0.491118, Valid_Cost:0.465700, Test_Cost:0.432930\n",
      "Best Epoch:4, Avg_Duration:13.110100, Train_Cost:0.491118, Valid_Cost:0.465700, Test_Cost:0.432930\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gram_hf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tseq_file = 'output2/output2.seqs'\n",
    "\ttree_file = 'output2/output2'\n",
    "\tlabel_file = 'output/output.hfs'\n",
    "\tout_file = 'ipynb/gram_hf'\n",
    "\tinputDimSize = gram_hf.calculate_dimSize(seq_file)\n",
    "\tnumClass = 1 # predicte a binary value (heart failure)\n",
    "\tnumAncestors = gram_hf.get_rootCode(tree_file+'.level2.pk') - inputDimSize + 1\n",
    "\n",
    "\tgram_hf.train_GRAM(\n",
    "\t\tseqFile=seq_file,\n",
    "\t\tinputDimSize=inputDimSize,\n",
    "\t\ttreeFile=tree_file,\n",
    "\t\tnumAncestors=numAncestors,\n",
    "\t\tlabelFile=label_file,\n",
    "\t\tnumClass=numClass,\n",
    "\t\toutFile=out_file,\n",
    "\t\tembFile='',\n",
    "\t\tembDimSize=128,\n",
    "\t\thiddenDimSize=128,\n",
    "\t\tattentionDimSize=128,\n",
    "\t\tbatchSize=100,\n",
    "\t\tmax_epochs=5,\n",
    "\t\tL2=0.001,\n",
    "\t\tdropoutRate=0.5,\n",
    "\t\tlogEps=1e-8,\n",
    "\t\ttrain_ratio=0.7\n",
    "\t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implemented the GRAM model without the attention mechanism in `gram_hf_no_attention.py`. The following code imported it as `gram_hf_no_attention` and train the model with 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model ...  done!!\n",
      "Constructing the optimizer ...  done!!\n",
      "Loading data ...  done!!\n",
      "Optimization start !!\n",
      "Epoch:0, Duration:2.722397, Train_Cost:0.516371, Valid_Cost:0.576333, Test_Cost:0.501965\n",
      "Epoch:1, Duration:2.729736, Train_Cost:0.479058, Valid_Cost:0.493964, Test_Cost:0.458061\n",
      "Epoch:2, Duration:2.700339, Train_Cost:0.441143, Valid_Cost:0.483906, Test_Cost:0.435884\n",
      "Epoch:3, Duration:2.674544, Train_Cost:0.399495, Valid_Cost:0.518985, Test_Cost:0.508718\n",
      "Epoch:4, Duration:2.727133, Train_Cost:0.379071, Valid_Cost:0.391843, Test_Cost:0.395522\n",
      "Best Epoch:4, Avg_Duration:2.710830, Train_Cost:0.379071, Valid_Cost:0.391843, Test_Cost:0.395522\n"
     ]
    }
   ],
   "source": [
    "import gram_hf_no_attention as gram_hf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tseq_file = 'output2/output2.seqs'\n",
    "\ttree_file = 'output2/output2'\n",
    "\tlabel_file = 'output/output.hfs'\n",
    "\tout_file = 'ipynb/gram_hf_no_attention' # for no attention\n",
    "\tinputDimSize = gram_hf.calculate_dimSize(seq_file)\n",
    "\tnumClass = 1 # predicte a binary value (heart failure)\n",
    "\tnumAncestors = gram_hf.get_rootCode(tree_file+'.level2.pk') - inputDimSize + 1\n",
    "\n",
    "\tgram_hf.train_GRAM(\n",
    "\t\tseqFile=seq_file,\n",
    "\t\tinputDimSize=inputDimSize,\n",
    "\t\ttreeFile=tree_file,\n",
    "\t\tnumAncestors=numAncestors,\n",
    "\t\tlabelFile=label_file,\n",
    "\t\tnumClass=numClass,\n",
    "\t\toutFile=out_file,\n",
    "\t\tembFile='',\n",
    "\t\tembDimSize=128,\n",
    "\t\thiddenDimSize=128,\n",
    "\t\tattentionDimSize=128,\n",
    "\t\tbatchSize=100,\n",
    "\t\tmax_epochs=5,\n",
    "\t\tL2=0.001,\n",
    "\t\tdropoutRate=0.5,\n",
    "\t\tlogEps=1e-8,\n",
    "\t\ttrain_ratio=0.7\n",
    "\t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated AUC for each model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GRAM with attention AUC:', 0.8514355428251791)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "with open('./ipynb/gram_hf_0.7.test_set', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open('./ipynb/gram_hf_0.7_4.test_probs', 'rb') as f:\n",
    "    y_pred_probs = pickle.load(f)\n",
    "\n",
    "y_true_labels = test_data[1]\n",
    "auc = roc_auc_score(y_true_labels, y_pred_probs)\n",
    "\n",
    "print(\"GRAM with attention AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GRAM without attention AUC:', 0.8606023782657755)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "with open('./ipynb/gram_hf_no_attention_0.7.test_set', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open('./ipynb/gram_hf_no_attention_0.7_4.test_probs', 'rb') as f:\n",
    "    y_pred_probs = pickle.load(f)\n",
    "\n",
    "y_true_labels = test_data[1]\n",
    "auc = roc_auc_score(y_true_labels, y_pred_probs)\n",
    "\n",
    "print(\"GRAM without attention AUC:\", auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the key results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and Jimeng Sun. 2017. Gram: graph-based attention model for healthcare represen-tation learning. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 787–795.\n",
    "- Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
